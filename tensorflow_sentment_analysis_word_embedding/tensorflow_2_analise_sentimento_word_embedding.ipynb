{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Weslley\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções que serão usadas durante o projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esta função apenas lê os dados do arquivo informado e retorna o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neste passo estamos criando uma funcao load_doc para tokenizar os textos\n",
    "A funcao clean_doc efetua os seguintes tratamentos: \n",
    "1. tokeniza o documento usando espaco; \n",
    "2. Remove pontuacao de cada token; \n",
    "3. Remove caracteres especiais de cada token;\n",
    "4. Remove stop words (ex.: de, para, do...);\n",
    "5. Remove tokens cujo tamanho seja <= 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    \n",
    "    # Separa as palavras (tokens) sempre que encontrar um espaço em branco\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # Remove pontuação\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Remove tokens não alfabéticos\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # Remove tokens com um caracter ou menos\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esta função carrega o documento, limpa o texto e remove palavras que não pertencem ao vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_line(filename, vocab):\n",
    "    \n",
    "    # Carrega o documento\n",
    "    doc = load_doc(filename)\n",
    "    \n",
    "    # Limpa o texto\n",
    "    tokens = clean_doc(doc)\n",
    "    \n",
    "    # Remove palavras que não pertencem ao vocabulário\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esta função é responsável por correr um diretório e carregar todos os arquivos que serão usados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    \n",
    "    lines = list()\n",
    "    \n",
    "    # Para cada arquivo existente no diretório\n",
    "    for filename in listdir(directory):\n",
    "    \n",
    "        # Verifica se deve carregar os dados de treino ou teste (pula o arquivo que não pertence ao conjunto em questão)\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        # Seta o diretório do arquivo\n",
    "        path = directory + '/' + filename\n",
    "        \n",
    "        # Carrega os dados\n",
    "        line = doc_to_line(path, vocab)\n",
    "        \n",
    "        # Adiciona aos resultados\n",
    "        lines.append(line)\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esta função salva o vocabulário em disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    \n",
    "    # Converte os tokens em string\n",
    "    data = '\\n'.join(lines)\n",
    "    \n",
    "    # Salva o vocabulário em um arquivo\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo o vocabulário\n",
    "Dados em http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estas funções criam o vocabulário de forma manual (sem uso de bibliotecas específicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary (directory, vocab):\n",
    "    \n",
    "    # Para cada arquivo existente no diretório\n",
    "    for filename in listdir(directory):\n",
    "        \n",
    "        # Pula os arquivos que pertencem ao conjunto de teste\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        # Prepara o path do arquivo\n",
    "        path = directory + '/' + filename\n",
    "        \n",
    "        # Adiciona texto ao vocabulário\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    \n",
    "    # Carrega o documento\n",
    "    doc = load_doc(filename)\n",
    "    \n",
    "    # Limpa o texto\n",
    "    tokens = clean_doc(doc)\n",
    "    \n",
    "    # Atualiza o vocabulário\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria o vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O vocabulário possui 44276 palavras\n",
      "\n",
      "As palavras mais comumns são:\n",
      "\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "# Nosso vocabulário será armazenado nesta variável\n",
    "vocab = Counter()\n",
    "\n",
    "# add all docs to vocab\n",
    "create_vocabulary('dataset/txt_sentoken/pos', vocab)\n",
    "create_vocabulary('dataset/txt_sentoken/neg', vocab)\n",
    "\n",
    "# print the size of the vocab\n",
    "print('O vocabulário possui {} palavras\\n'.format(len(vocab)))\n",
    "\n",
    "# print the top words in the vocab\n",
    "print('As palavras mais comumns são:\\n\\n{}'.format(vocab.most_common(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora vamos atualizar nosso vocabulario e manter apenas os tokens com mais de 1 ocorrencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O novo vocabulário possui 25767 palavras\n"
     ]
    }
   ],
   "source": [
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print('O novo vocabulário possui {} palavras'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente vamos salvar nosso vocabulario em disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o vocabulário em um arquivo\n",
    "save_list(tokens, 'dataset/txt_sentoken/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o vocabulário\n",
    "vocab_filename = 'dataset/txt_sentoken/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'every movie comes along suspect studio every indication stinker everybodys surprise perhaps even studio film becomes critical darling mtv films election high school comedy starring matthew broderick reese witherspoon current example anybody know film existed week opened plot deceptively simple george washington carver high school student elections tracy flick reese witherspoon overachiever hand raised nearly every question way way high mr matthew broderick sick megalomaniac student encourages paul jock run pauls nihilistic sister jumps race well personal reasons dark side sleeper success expectations low going fact quality stuff made reviews even enthusiastic right cant help going baggage glowing reviews contrast negative baggage reviewers likely election good film live hype makes election disappointing contains significant plot details lifted directly rushmore released months earlier similarities staggering tracy flick election president extraordinary number clubs involved school play max fischer rushmore president extraordinary number clubs involved school play significant tension election potential relationship teacher student significant tension rushmore potential relationship teacher student tracy flick single parent home contributed drive max fischer single parent home contributed drive male bumbling adult election matthew broderick pursues extramarital affair gets caught whole life ruined even gets bee sting male bumbling adult rushmore bill murray pursues extramarital affair gets caught whole life ruined gets several bee stings happened individual screenplay rushmore novel election contain many significant plot points yet films probably even aware made two different studios genre high school geeks revenge movie hadnt fully formed yet even strengths election rely upon fantastic performances broderick witherspoon newcomer jessica campbell pauls antisocial sister tammy broderick playing mr rooney role ferris bueller seems fun hes since witherspoon revelation early year comedy teenagers little clout money witherspoon deserves oscar nomination campbells character gets going like fantastic speech gymnasium youre one thing thats bothering since ive seen extraordinary amount sexuality film suppose coming mtv films expect less film starts light airy like sitcom screws tighten tensions mount alexander payne decides add elements frankly distract story bad enough mr doesnt like determination win costs throw relationship even theres logical reason mr affair theres lot like election plot similarities rushmore nosedive takes gets explicitly sexdriven mark disappointment'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega os dados de treino\n",
    "positive_lines = process_docs('dataset/txt_sentoken/pos', vocab, True)\n",
    "negative_lines = process_docs('dataset/txt_sentoken/neg', vocab, True)\n",
    "docs_train = negative_lines + positive_lines\n",
    "\n",
    "# Seta a variável target (sabemos que os primeiros 900 textos são comentários negativos e os outros 900 são positivos)\n",
    "ytrain = np.array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "\n",
    "# Exibe um exemplo\n",
    "print(len(positive_lines), len(negative_lines))\n",
    "positive_lines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'start movie reminded parts movie stargate people looking around egyptian temple reading dangerous thing going destroy earth future sort confusing bit involving fakelooking cyborg things movie jumps future movie improves leaps bounds basic idea behind movie every make every years evil force comes destroy earth things needed defend menace four elements nature plus fifth element plot movie really isnt important thing though movie good special effects part music background fits mood well bruce willis illegal driver futuristic new york city one day lady bandages drops trunk movie happens plot twists interesting movie never fails present viewer variety different locations also fair bit action film particularly towards end characters plain strange including deejay drag bruce willis normal job blowing things away like always movie definitely watchable rarely slows one scifi films youll saying cool followed hell give fifth element'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all test reviews\n",
    "positive_lines = process_docs('dataset/txt_sentoken/pos', vocab, False)\n",
    "negative_lines = process_docs('dataset/txt_sentoken/neg', vocab, False)\n",
    "docs_test = negative_lines + positive_lines\n",
    "\n",
    "# Seta a variável target (sabemos que os primeiros 100 textos são comentários negativos e os outros 100 são positivos)\n",
    "ytest = np.array([0 for _ in range(100)] + [1 for _ in range(100)]) # os primeiros sempre sao da classe pos.\n",
    "\n",
    "# Exibe um exemplo\n",
    "print(len(positive_lines), len(negative_lines))\n",
    "positive_lines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumo\n",
    "Até este ponto fizemos os seguintes passos:\n",
    "* Limpeza dos dados\n",
    "* Criação de um vocabulário\n",
    "\n",
    "Agora já é possível receber um texto qualquer e, por meio das funções que criamos, fazer a limpeza deste texto e restringi-lo ao vocabulário que criamos com os dados de teste. Daqui pra frente vamos explorar diferentes maneiras de transformar o nosso texto (que já está limpo) e uma **representação numérica**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o modelo com embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforma texto em números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do vocabulário\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Cria e treina o tokenizer\n",
    "tokenizer = Tokenizer(num_words= vocab_size, filters='')\n",
    "tokenizer.fit_on_texts(docs_train)\n",
    "\n",
    "# Transforma os dados de treino\n",
    "encoded_docs = tokenizer.texts_to_sequences(docs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usa um vetor de mesmo tamanho para representar todos os textos (completa os vetores menores com zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 0\n",
    "for enc in encoded_docs:\n",
    "    max_length = max(max_length, len(enc))\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define e compila o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 1317, 8)           206136    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 10536)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                526850    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 733,037\n",
      "Trainable params: 733,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='sigmoid'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treina o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1350 samples, validate on 450 samples\n",
      "Epoch 1/100\n",
      "1350/1350 [==============================] - 1s 1ms/sample - loss: 0.6556 - acc: 0.6556 - val_loss: 1.0979 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1350/1350 [==============================] - 1s 660us/sample - loss: 0.6015 - acc: 0.6815 - val_loss: 1.2183 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1350/1350 [==============================] - 1s 648us/sample - loss: 0.5023 - acc: 0.7400 - val_loss: 1.1498 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1350/1350 [==============================] - 1s 648us/sample - loss: 0.3039 - acc: 0.9348 - val_loss: 1.1994 - val_acc: 0.0133\n",
      "Epoch 5/100\n",
      "1350/1350 [==============================] - 1s 678us/sample - loss: 0.1397 - acc: 0.9919 - val_loss: 1.1222 - val_acc: 0.0822\n",
      "Epoch 6/100\n",
      "1350/1350 [==============================] - 1s 648us/sample - loss: 0.0663 - acc: 1.0000 - val_loss: 1.2962 - val_acc: 0.0756\n",
      "Epoch 7/100\n",
      "1350/1350 [==============================] - 1s 666us/sample - loss: 0.0388 - acc: 1.0000 - val_loss: 1.3154 - val_acc: 0.1133\n",
      "Epoch 8/100\n",
      "1350/1350 [==============================] - 1s 734us/sample - loss: 0.0250 - acc: 1.0000 - val_loss: 1.3710 - val_acc: 0.1267\n",
      "Epoch 9/100\n",
      "1350/1350 [==============================] - 1s 764us/sample - loss: 0.0183 - acc: 1.0000 - val_loss: 1.4259 - val_acc: 0.1289\n",
      "Epoch 10/100\n",
      "1350/1350 [==============================] - 1s 672us/sample - loss: 0.0138 - acc: 1.0000 - val_loss: 1.4806 - val_acc: 0.1378\n",
      "Epoch 11/100\n",
      "1350/1350 [==============================] - 1s 648us/sample - loss: 0.0107 - acc: 1.0000 - val_loss: 1.4687 - val_acc: 0.1667\n",
      "Epoch 12/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 0.0092 - acc: 1.0000 - val_loss: 1.5055 - val_acc: 0.1711\n",
      "Epoch 13/100\n",
      "1350/1350 [==============================] - 1s 810us/sample - loss: 0.0076 - acc: 1.0000 - val_loss: 1.5508 - val_acc: 0.1733\n",
      "Epoch 14/100\n",
      "1350/1350 [==============================] - 1s 752us/sample - loss: 0.0064 - acc: 1.0000 - val_loss: 1.5523 - val_acc: 0.1756\n",
      "Epoch 15/100\n",
      "1350/1350 [==============================] - 1s 670us/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 1.5795 - val_acc: 0.1756\n",
      "Epoch 16/100\n",
      "1350/1350 [==============================] - 1s 660us/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 1.5875 - val_acc: 0.1778\n",
      "Epoch 17/100\n",
      "1350/1350 [==============================] - 1s 733us/sample - loss: 0.0047 - acc: 1.0000 - val_loss: 1.6248 - val_acc: 0.1822\n",
      "Epoch 18/100\n",
      "1350/1350 [==============================] - 1s 845us/sample - loss: 0.0040 - acc: 1.0000 - val_loss: 1.6249 - val_acc: 0.1867\n",
      "Epoch 19/100\n",
      "1350/1350 [==============================] - 1s 850us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 1.6531 - val_acc: 0.1844\n",
      "Epoch 20/100\n",
      "1350/1350 [==============================] - 1s 822us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 1.6409 - val_acc: 0.1889\n",
      "Epoch 21/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 1.6757 - val_acc: 0.1889\n",
      "Epoch 22/100\n",
      "1350/1350 [==============================] - 1s 698us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 1.6683 - val_acc: 0.1889\n",
      "Epoch 23/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 1.6921 - val_acc: 0.1889\n",
      "Epoch 24/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 1.7177 - val_acc: 0.1889\n",
      "Epoch 25/100\n",
      "1350/1350 [==============================] - 1s 671us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 1.7411 - val_acc: 0.1889\n",
      "Epoch 26/100\n",
      "1350/1350 [==============================] - 1s 880us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 1.7666 - val_acc: 0.1889\n",
      "Epoch 27/100\n",
      "1350/1350 [==============================] - 1s 880us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 1.7805 - val_acc: 0.1956\n",
      "Epoch 28/100\n",
      "1350/1350 [==============================] - 1s 746us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 1.7903 - val_acc: 0.1978\n",
      "Epoch 29/100\n",
      "1350/1350 [==============================] - 1s 671us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 1.7926 - val_acc: 0.2022\n",
      "Epoch 30/100\n",
      "1350/1350 [==============================] - 1s 787us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 1.8150 - val_acc: 0.2022\n",
      "Epoch 31/100\n",
      "1350/1350 [==============================] - 1s 704us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 1.8048 - val_acc: 0.2089\n",
      "Epoch 32/100\n",
      "1350/1350 [==============================] - 1s 706us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 1.7980 - val_acc: 0.2178\n",
      "Epoch 33/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 1.8290 - val_acc: 0.2111\n",
      "Epoch 34/100\n",
      "1350/1350 [==============================] - 1s 814us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 1.8439 - val_acc: 0.2111\n",
      "Epoch 35/100\n",
      "1350/1350 [==============================] - 1s 694us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 1.8441 - val_acc: 0.2244\n",
      "Epoch 36/100\n",
      "1350/1350 [==============================] - 1s 729us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 1.8573 - val_acc: 0.2267\n",
      "Epoch 37/100\n",
      "1350/1350 [==============================] - 1s 691us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 1.8493 - val_acc: 0.2333\n",
      "Epoch 38/100\n",
      "1350/1350 [==============================] - 1s 741us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 1.8536 - val_acc: 0.2378\n",
      "Epoch 39/100\n",
      "1350/1350 [==============================] - 1s 721us/sample - loss: 9.9177e-04 - acc: 1.0000 - val_loss: 1.8433 - val_acc: 0.2400\n",
      "Epoch 40/100\n",
      "1350/1350 [==============================] - 1s 726us/sample - loss: 9.5053e-04 - acc: 1.0000 - val_loss: 1.8863 - val_acc: 0.2333\n",
      "Epoch 41/100\n",
      "1350/1350 [==============================] - 1s 743us/sample - loss: 9.3697e-04 - acc: 1.0000 - val_loss: 1.9080 - val_acc: 0.2333\n",
      "Epoch 42/100\n",
      "1350/1350 [==============================] - 1s 961us/sample - loss: 8.3428e-04 - acc: 1.0000 - val_loss: 1.9184 - val_acc: 0.2333\n",
      "Epoch 43/100\n",
      "1350/1350 [==============================] - 1s 679us/sample - loss: 8.0443e-04 - acc: 1.0000 - val_loss: 1.9148 - val_acc: 0.2378\n",
      "Epoch 44/100\n",
      "1350/1350 [==============================] - 1s 706us/sample - loss: 8.1021e-04 - acc: 1.0000 - val_loss: 1.9448 - val_acc: 0.2333\n",
      "Epoch 45/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 8.0159e-04 - acc: 1.0000 - val_loss: 1.9546 - val_acc: 0.2356\n",
      "Epoch 46/100\n",
      "1350/1350 [==============================] - 1s 876us/sample - loss: 6.6954e-04 - acc: 1.0000 - val_loss: 1.9562 - val_acc: 0.2378\n",
      "Epoch 47/100\n",
      "1350/1350 [==============================] - 1s 942us/sample - loss: 6.5480e-04 - acc: 1.0000 - val_loss: 1.9470 - val_acc: 0.2400\n",
      "Epoch 48/100\n",
      "1350/1350 [==============================] - 1s 764us/sample - loss: 6.7594e-04 - acc: 1.0000 - val_loss: 1.9484 - val_acc: 0.2422\n",
      "Epoch 49/100\n",
      "1350/1350 [==============================] - 1s 869us/sample - loss: 6.8273e-04 - acc: 1.0000 - val_loss: 1.9458 - val_acc: 0.2422\n",
      "Epoch 50/100\n",
      "1350/1350 [==============================] - 1s 848us/sample - loss: 6.3820e-04 - acc: 1.0000 - val_loss: 1.9627 - val_acc: 0.2444\n",
      "Epoch 51/100\n",
      "1350/1350 [==============================] - 1s 860us/sample - loss: 5.9005e-04 - acc: 1.0000 - val_loss: 2.0058 - val_acc: 0.2400\n",
      "Epoch 52/100\n",
      "1350/1350 [==============================] - 1s 729us/sample - loss: 5.9237e-04 - acc: 1.0000 - val_loss: 1.9925 - val_acc: 0.2467\n",
      "Epoch 53/100\n",
      "1350/1350 [==============================] - 1s 800us/sample - loss: 5.6871e-04 - acc: 1.0000 - val_loss: 2.0004 - val_acc: 0.2489\n",
      "Epoch 54/100\n",
      "1350/1350 [==============================] - 1s 845us/sample - loss: 5.0236e-04 - acc: 1.0000 - val_loss: 2.0286 - val_acc: 0.2444\n",
      "Epoch 55/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 4.9348e-04 - acc: 1.0000 - val_loss: 2.0195 - val_acc: 0.2467\n",
      "Epoch 56/100\n",
      "1350/1350 [==============================] - 1s 696us/sample - loss: 5.0940e-04 - acc: 1.0000 - val_loss: 2.0116 - val_acc: 0.2489\n",
      "Epoch 57/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 4.5972e-04 - acc: 1.0000 - val_loss: 2.0306 - val_acc: 0.2489\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 1s 671us/sample - loss: 4.4894e-04 - acc: 1.0000 - val_loss: 2.0307 - val_acc: 0.2533\n",
      "Epoch 59/100\n",
      "1350/1350 [==============================] - 1s 662us/sample - loss: 4.2888e-04 - acc: 1.0000 - val_loss: 2.0346 - val_acc: 0.2556\n",
      "Epoch 60/100\n",
      "1350/1350 [==============================] - 1s 683us/sample - loss: 4.4621e-04 - acc: 1.0000 - val_loss: 2.0455 - val_acc: 0.2533\n",
      "Epoch 61/100\n",
      "1350/1350 [==============================] - 1s 766us/sample - loss: 4.1225e-04 - acc: 1.0000 - val_loss: 2.0580 - val_acc: 0.2511\n",
      "Epoch 62/100\n",
      "1350/1350 [==============================] - 1s 828us/sample - loss: 3.8554e-04 - acc: 1.0000 - val_loss: 2.0740 - val_acc: 0.2511\n",
      "Epoch 63/100\n",
      "1350/1350 [==============================] - 1s 799us/sample - loss: 4.0931e-04 - acc: 1.0000 - val_loss: 2.0746 - val_acc: 0.2556\n",
      "Epoch 64/100\n",
      "1350/1350 [==============================] - 1s 734us/sample - loss: 3.5226e-04 - acc: 1.0000 - val_loss: 2.0884 - val_acc: 0.2556\n",
      "Epoch 65/100\n",
      "1350/1350 [==============================] - 1s 694us/sample - loss: 3.6417e-04 - acc: 1.0000 - val_loss: 2.1122 - val_acc: 0.2533\n",
      "Epoch 66/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 3.6596e-04 - acc: 1.0000 - val_loss: 2.1226 - val_acc: 0.2556\n",
      "Epoch 67/100\n",
      "1350/1350 [==============================] - 1s 722us/sample - loss: 3.7778e-04 - acc: 1.0000 - val_loss: 2.1128 - val_acc: 0.2622\n",
      "Epoch 68/100\n",
      "1350/1350 [==============================] - 1s 696us/sample - loss: 3.5625e-04 - acc: 1.0000 - val_loss: 2.1057 - val_acc: 0.2667\n",
      "Epoch 69/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 3.2672e-04 - acc: 1.0000 - val_loss: 2.1004 - val_acc: 0.2711\n",
      "Epoch 70/100\n",
      "1350/1350 [==============================] - 1s 706us/sample - loss: 3.3608e-04 - acc: 1.0000 - val_loss: 2.0786 - val_acc: 0.2733\n",
      "Epoch 71/100\n",
      "1350/1350 [==============================] - 1s 683us/sample - loss: 3.1838e-04 - acc: 1.0000 - val_loss: 2.0977 - val_acc: 0.2711\n",
      "Epoch 72/100\n",
      "1350/1350 [==============================] - 1s 888us/sample - loss: 2.9578e-04 - acc: 1.0000 - val_loss: 2.1265 - val_acc: 0.2711\n",
      "Epoch 73/100\n",
      "1350/1350 [==============================] - 1s 729us/sample - loss: 2.9146e-04 - acc: 1.0000 - val_loss: 2.1478 - val_acc: 0.2667\n",
      "Epoch 74/100\n",
      "1350/1350 [==============================] - 1s 698us/sample - loss: 2.6361e-04 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.2667\n",
      "Epoch 75/100\n",
      "1350/1350 [==============================] - 1s 702us/sample - loss: 2.7263e-04 - acc: 1.0000 - val_loss: 2.1626 - val_acc: 0.2667\n",
      "Epoch 76/100\n",
      "1350/1350 [==============================] - 1s 718us/sample - loss: 2.6633e-04 - acc: 1.0000 - val_loss: 2.1615 - val_acc: 0.2689\n",
      "Epoch 77/100\n",
      "1350/1350 [==============================] - 1s 723us/sample - loss: 2.4472e-04 - acc: 1.0000 - val_loss: 2.1842 - val_acc: 0.2667\n",
      "Epoch 78/100\n",
      "1350/1350 [==============================] - 1s 706us/sample - loss: 2.4389e-04 - acc: 1.0000 - val_loss: 2.1907 - val_acc: 0.2667\n",
      "Epoch 79/100\n",
      "1350/1350 [==============================] - 1s 686us/sample - loss: 2.5560e-04 - acc: 1.0000 - val_loss: 2.2137 - val_acc: 0.2667\n",
      "Epoch 80/100\n",
      "1350/1350 [==============================] - 1s 683us/sample - loss: 2.1763e-04 - acc: 1.0000 - val_loss: 2.2245 - val_acc: 0.2667\n",
      "Epoch 81/100\n",
      "1350/1350 [==============================] - 1s 914us/sample - loss: 2.5268e-04 - acc: 1.0000 - val_loss: 2.2094 - val_acc: 0.2689\n",
      "Epoch 82/100\n",
      "1350/1350 [==============================] - 1s 841us/sample - loss: 2.1712e-04 - acc: 1.0000 - val_loss: 2.2291 - val_acc: 0.2667\n",
      "Epoch 83/100\n",
      "1350/1350 [==============================] - 1s 919us/sample - loss: 1.9745e-04 - acc: 1.0000 - val_loss: 2.2596 - val_acc: 0.2644\n",
      "Epoch 84/100\n",
      "1350/1350 [==============================] - 1s 833us/sample - loss: 2.0945e-04 - acc: 1.0000 - val_loss: 2.2583 - val_acc: 0.2667\n",
      "Epoch 85/100\n",
      "1350/1350 [==============================] - 1s 738us/sample - loss: 2.2292e-04 - acc: 1.0000 - val_loss: 2.2605 - val_acc: 0.2667\n",
      "Epoch 86/100\n",
      "1350/1350 [==============================] - 1s 694us/sample - loss: 2.0148e-04 - acc: 1.0000 - val_loss: 2.2588 - val_acc: 0.2689\n",
      "Epoch 87/100\n",
      "1350/1350 [==============================] - 1s 683us/sample - loss: 1.9282e-04 - acc: 1.0000 - val_loss: 2.3214 - val_acc: 0.2622\n",
      "Epoch 88/100\n",
      "1350/1350 [==============================] - 1s 833us/sample - loss: 1.9025e-04 - acc: 1.0000 - val_loss: 2.3178 - val_acc: 0.2644\n",
      "Epoch 89/100\n",
      "1350/1350 [==============================] - 1s 836us/sample - loss: 1.9018e-04 - acc: 1.0000 - val_loss: 2.2872 - val_acc: 0.2689\n",
      "Epoch 90/100\n",
      "1350/1350 [==============================] - 1s 873us/sample - loss: 1.9329e-04 - acc: 1.0000 - val_loss: 2.2695 - val_acc: 0.2733\n",
      "Epoch 91/100\n",
      "1350/1350 [==============================] - 1s 810us/sample - loss: 1.6981e-04 - acc: 1.0000 - val_loss: 2.2854 - val_acc: 0.2733\n",
      "Epoch 92/100\n",
      "1350/1350 [==============================] - 1s 810us/sample - loss: 1.8033e-04 - acc: 1.0000 - val_loss: 2.3094 - val_acc: 0.2733\n",
      "Epoch 93/100\n",
      "1350/1350 [==============================] - 1s 845us/sample - loss: 1.7569e-04 - acc: 1.0000 - val_loss: 2.3141 - val_acc: 0.2733\n",
      "Epoch 94/100\n",
      "1350/1350 [==============================] - 1s 750us/sample - loss: 1.7711e-04 - acc: 1.0000 - val_loss: 2.2957 - val_acc: 0.2733\n",
      "Epoch 95/100\n",
      "1350/1350 [==============================] - 1s 868us/sample - loss: 1.4496e-04 - acc: 1.0000 - val_loss: 2.3227 - val_acc: 0.2733\n",
      "Epoch 96/100\n",
      "1350/1350 [==============================] - 1s 712us/sample - loss: 1.5239e-04 - acc: 1.0000 - val_loss: 2.3471 - val_acc: 0.2689\n",
      "Epoch 97/100\n",
      "1350/1350 [==============================] - 1s 856us/sample - loss: 1.5424e-04 - acc: 1.0000 - val_loss: 2.3594 - val_acc: 0.2689\n",
      "Epoch 98/100\n",
      "1350/1350 [==============================] - 1s 741us/sample - loss: 1.4061e-04 - acc: 1.0000 - val_loss: 2.3734 - val_acc: 0.2689\n",
      "Epoch 99/100\n",
      "1350/1350 [==============================] - 1s 834us/sample - loss: 1.4273e-04 - acc: 1.0000 - val_loss: 2.3638 - val_acc: 0.2733\n",
      "Epoch 100/100\n",
      "1350/1350 [==============================] - 1s 752us/sample - loss: 1.4350e-04 - acc: 1.0000 - val_loss: 2.3654 - val_acc: 0.2733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24006fe7ac8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, ytrain, epochs=100, verbose=1, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avalia o modelo nos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.000000\n"
     ]
    }
   ],
   "source": [
    "# Transforma strings em números\n",
    "encoded_docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "\n",
    "# Preenche os vetores menores com zeros\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Avalia o modelo nos dados de teste\n",
    "loss, accuracy = model.evaluate(padded_docs_test, ytest, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "* https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "* https://www.tensorflow.org/hub\n",
    "* https://tfhub.dev/google/nnlm-en-dim128/2\n",
    "* https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
